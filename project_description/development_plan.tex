\section{Development Plan}

The success of the \grackle{} project as a community resource will
depend on its ability to adapt to the evolving scientific and
technical needs of the users. Key to this are 1) adding features
desired by users, 2) encouraging external contribution by ensuring
that the code is easily extendable, and 3) improving performance to
keep up with advances in the simulation codes that use it.
The proposed work plan, detailed below, will address each of these key
factors. Each year of the work plan has milestones in the form of
deliverables to the community which will enable specific scientific
goals. The features and improvements that we will implement were all
identified in a \grackle{} user survey carried out in Spring 2017 as
being important to the community. In addition, we identify scientific
collaborators with stated intentions of making use of the new features
developed. Figure \ref{fig:gantt} displays the timeline for the
design, implementation, and delivery of each of the milestones in this
CSSI proposal. \grackle{} and \dengo{} are both distributed under the
3-clause, revised BSD license and have no proprietary
dependencies. The planning and commencement of this work will include
direct communication with the \grackle{} user community via the
project's mailing list.

\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/gantt.pdf}
\caption{Timeline of design, implementation, and delivery of the
  proposed development.}
\label{fig:gantt}
\end{center}
\vspace*{-2\baselineskip}
\end{figure}

\subsection{Year One: \grackle{}: source code refactor and hardware
  acceleration; \dengo{}: explicit solver generation}

The three objectives for year one are 1) to make \grackle{} subsantially
easier to modify and, hence, more inviting to new contributions, 2)
add generalized support for hardware accelerators to \grackle{}, and
3) to add support for generating explicit solvers to \dengo{}.  The
first two goals will result in a significantly improved \grackle{}
library ready at the end of year one. All three objectives will
contribute to the foundation of the work to be done in years two and
three.

\noindent \textbf{Grackle Source Code Refactor}
The design and optimization of the core chemistry and cooling solver
routines in \grackle{} are central factors in making the library fast
enough to be used in extremely large simulations.  However, these
factors also make this the most challenging area of \grackle{} to
work with as a developer.  For example, because there is no simple way
of sharing common/global data or variables between the C and Fortran
routines, the Fortran routines must be handed all parameters, fields,
and rate data as individual arguments.  This results in intimidating
function signatures with dozens of arguments and is only made worse by
the lack of consistency checking between the C and Fortran argument
lists during compilation.  Inconsistencies will result in segmentation
faults whose debugging requires comparison of long lists of function
arguments.  Additional difficulties also include maintaining
consistency in variable type across the C/Fortran interface and the
general confusion that can arise from working in two different
languages at once.  These factors increase the likelihood of bugs and
make the source code significantly less inviting for new
contributors.

The primary advantage of using Fortran for \grackle{}'s most
computationally expensive routines is the speed of array operations.
This is extremely important for \grackle{}, as the code is designed to
work on multiple, large arrays of fields.  This speed advantage was
especially great when the core solvers were originally developed in
the mid-1990s.  However, since that time, advances in the C
programming language, such as the \texttt{restrict} keyword introduced
in the C99 standard \citep{c99}, have made it possible for C code to
be competitive with Fortran.

The source code refactor will begin with an expansion of the
test suite to ensure that all functionality and parameter
combinations are covered by answer and unit tests.  The current
test suite adequately covers the most common running modes of the
library, but is insufficient for a full overhaul such as what will
follow.  With a more robust test suite in place, the Fortran code (roughly 7200
lines) can be safely ported to C.  The resulting code will
be vastly streamlined as it will no longer be necessary to pass all
required data via function argument.  Instead, the central routines
will be able to access the C \texttt{structs} containing the run-time
parameters, rate data, and field arrays.  In addition, the ease of
calling C routines from Python will allow for the addition of
finer-grained unit tests operating within the existing test suite,
which uses the \texttt{pytest} Python package.  This will also allow
for more of the core functionality to be exposed directly to the user,
as has often been requested.  The culmination of this phase of the
work plan will be marked by a major public release, \grackle{} version
4.0.

%% At present, on startup \grackle{} computes lookup tables from the
%% functional forms of the reaction rate coefficients.  To enable easier
%% selection, and greater provenance tracking, we will enable the
%% \texttt{pygrackle} wrappers to generate pre-computed tables from
%% functional forms defined in yaml files.  This will provide a method of
%% experimenting with different rates, such as from different
%% experimental results or theoretical calculations, while avoiding the
%% complexity of code generation or full recompilation of the
%% application.  Finally, with this refactor, the
%% increased modularity will allow for the implementation of different
%% solver methodologies.  One potential application of this could be the
%% fully-implicit solver LSODE \citep{LSODE}.  While LSODE is much slower in many
%% cases, it allows the individual to specify a more strict convergence
%% criteria and error threshold than the existing method in \grackle{}.
%% This may provide utility for situations where \grackle{} struggles
%% with time-step convergence, such as at extremely high densities.
%% The result of phase 2 will be a streamlined, more flexible,
%% refactored source code that will be substantially easier to develop
%% and will be released as \grackle{} version 4.0.

\noindent \textbf{Grackle Hardware Acceleration}
As astrophysical simulation codes continue to improve their
scalability and performance, it is important that \grackle{} not
become a limiting factor.  \grackle{}-related calculations are
generally known to constitute 20-25\% of a simulation's computational
cost.  Thus, the potential exists for \grackle{} to serve as a great
help or hindrance to continued performance gains.  Because the
calculations performed by \grackle{} are all local to each fluid
element (i.e., solving a given cell does not require information about
neighboring cells), it was straightforward to achieve modest speedup
with OpenMP simply by threading the outer loops for which the core
functions are called.  However, significantly greater speedups can be
achieved by parallelizing the operations within the solvers and taking
advantage of hardware acceleration options currently available on the
latest high performance computing (HPC) systems, mainly graphics
processing units (GPUs). For example, \citet{Haidar2016PerformanceAA}
report speedups of a factor of 20-40 for an explicit chemistry network
ported to a GPU and even greater speedups when the GPU is used to
compute multiple networks simultaneously.

National computing facilities used for astrophysical simulations, such
as those in the NSF-funded Extreme Science and Engineering Discovery
Environment (XSEDE) and NCSA Blue Waters, employ a variety of
architectures.  Implementing targeted parallelization strategies,
while potentially resulting in faster code, will make the source code
more complex. Instead, we will develop a new parallelism strategy
using the OpenACC standard, which works with many different
architectures.  OpenACC is supported 
by PGI, Cray, and GNU compilers.  Similar to OpenMP, OpenACC provides
compiler directives, or pragmas, that allow the programmer to
designate portions of code, particularly loops, to be run on the
accelerator.  Additional directives provide control of when data is
copied to and from the accelerator and also to allow certain data to
exist only in the accelerator's memory.  This strategy is well-suited
to \grackle{}'s core functions which make use of a number of temporary
variables and data to perform many operations on a series of input
arrays.

Work on OpenACC support will commence immediately following the
refactor of the \grackle{} source code, with the ease of adding
OpenACC support motivating the redesign of the code.  We will
make use of local computing resources at PI Smith's home institution
(San Diego Supercomputer Center, SDSC) for development and testing.
SDSC's Comet supercomputer has 36 NVIDIA dual K80 nodes and 36 NVIDIA
4-way P100 nodes. We will also work with system administrators at
XSEDE facilities to provide modules of pre-built \grackle{}
libraries. This phase of the project will be released as \grackle{}
version 4.1. Included in this release will be documentation aimed at
new developers with instructions for parallelizing new features.  To
support the broader impacts of this effort, we will also host a
workshop to provide instruction and a venue for collaboration for
researchers interested in contributing new features. This will be held
at the Center for Computational Astrophysics (CCA) at the Flatiron
Institute.  PI Smith will be responsible for this work.

\noindent \textbf{Dengo explicit solver generation}
\mjt{Help here, please!}

- we want to do this because expansion to larger chemistry networks is
fraught with peril and bugs

- to be carried out by PI Turk's research group in collaboration with
PI Smith.

- tuning experiments with grackle will guide development

\subsection{Year Two: \grackle{}: feature expansion; \dengo{}:
  hardware acceleration and reaction rate database support}

\textbf{Science Enabled:}

- Devin's non-equilibrium stuff goes faster

- new features from student project 1 (multi-element cooling or
rad-transfer cooling tables)

\subsection{Year Three: \grackle{}/\dengo{} hybrid solver and
  \grackle{} feature expansion}

\textbf{Science Enabled:}

THIS TEXT IS OLD BUT MIGHT BE REUSABLE

The goal of the year three development project is to expand the user
base of \grackle{} to include the present-day star formation
community.  
In contrast to the early Universe, where only H$_{2}$ is important,
star forming clouds in the local Universe are governed by a host of
additional processes, including heat input from cosmic rays and nearby
stars; cooling from molecules like CO, OH, and H$_{2}$O; cooling from
atomic C/O fine-structure emission; and both heating and cooling
effects from dust grains.  Following the detailed chemical structure
of these complex environments is currently outside the range of
\grackle{}'s capabilities.  However, detailed studies of the minimal
chemistry networks required for accurately modeling these systems
\citep{2012MNRAS.421..116G, 2017ApJ...843...38G} have paved the way
forward.  Expanding the chemistry solver to include additional species
will help to significantly grow the community of \grackle{} users.
This expansion was also the number one requested feature (by 16 out of
23 total respondents, representing 9 of the supported codes) in a
recent online survey of current \grackle{} users.

In year three, the existing primordial chemistry network will be
expanded to include the network of \citet{2017ApJ...843...38G}.  This
is an 18-species network that includes O, O$^{+}$, C, C$^{+}$, CO,
HCO$^{+}$, Si, Si$^{}$+, CH$_{x}$ and OH$_{x}$ (CH$_{x}$ and OH$_{x}$
are pseudo-molecules representing multiple molecules) in addition to
the species currently covered by \grackle{}.
\citet{2017ApJ...843...38G} show that this network out-performs all of
the minimal models discussed in \citep{2012MNRAS.421..116G} when
compared to the results of a more sophisticated photo-dissociation
region calculation.  The improvements made in years one and two will
make this effort significantly easier than if it were undertaken
first.  This will also provide a clear path for implementation of
additional networks in the future.  The year three project both adds
functionality and provides a template for user contributions.

This model will be added to \grackle{} using the parallelization
strategy implemented in year two and will be released as a 5.x version
of the \grackle{} library.

\subsection{Engineering and Release Process}

The \grackle{} project follows an established model for development
and releases that is based on other successful community software
projects in which \grackle{}'s developers participate.  The main
source code repository is hosted on
BitBucket.org\footnote{\url{https://bitbucket.org/grackle/grackle}}
and is publicly readable.  Development proceeds via a ``Pull Request''
(PR) model that allows for incoming changes to be effectively
peer-reviewed before  being accepted into the main branch.  The
\grackle{} library uses semantic versioning and
is released with numeric versions in an X.Y.Z format, e.g. ``version
3.2.1''. Changes to the \grackle{} API or a fundamental restructuring
of the source (such as the year one development project) constitute
changes to the X number in a release.  A change in Y occurs when new,
non-API-breaking features are added.  A change in Z marks a release
containing only bugfixes, but no new functionality.

This development process is primarily modelled after that
of the \yt{} community.  The \yt{} project has similar
aims as a cross-platform library for simulations and the PIs have
played integral roles in growing 
its community and crafting its governance model.  Similar to
\yt{}, the \grackle{} project maintains a Community Code of
Conduct and Developer Guide documents to promote a diverse and
inviting community.

\grackle{} is developed using the Mercurial distributed version
control system.  The main repository on BitBucket.org is publicly
readable with write-access currently granted to a handful of the most
experienced developers.  The Pull Request model is used even by those
with write access to the main repo.  In this model, a contributor
``forks'' the main repository, commits changes to their fork, and then
submits a PR allowing other developers to review the
line-by-line changes.  Reviewers leave comments or ``approve'' the
PR and it is eventually accepted or declined in a process
analogous to peer-review for publications.  At the present time, this
usually proceeds with the lead developer (PI Britton Smith) serving as
PR manager by identifying appropriate reviewers (including
himself) and soliciting comments.  As the community continues to grow,
this role can become codified and rotated amongst multiple people.
\grackle{} has received 76 PRs since the first release.

\grackle{} contains a suite of unit and answer
tests that can be run manually using the \texttt{pytest} package.  The
test suite is also run automatically on the central repository and its
forks using the Bitbucket Pipelines continuous integration service.
When a new PR is issued, the results of the test suite are
shown on the PR's web page with links to more detailed
output which can be examined when failures occur.  The test suite
includes a variety of different tests to guard against regression.
Several ``unit tests'' exist to ensure that fundamental
constraints are not violated.  For example, the cooling rates should
match for identical fluid containers that have different internal unit
systems or are in different cosmological reference frames (i.e.,
comoving or proper).  \grackle{} also has a series of ``answer
tests'', wherein the results of specific calculations are compared
against stored values from prior versions.  Finally, to ensure that
the library continues to function as advertised, the test suite
attempts to compile and run the example codes that demonstrate the
APIs.

\grackle{}'s documentation is included in the source and is written in
reStructuredText, which can be converted to HTML and PDF formats
using the Sphinx package.  An HTML rendering of the documentation is
also hosted on
readthedocs.org\footnote{\url{https://grackle.readthedocs.io}},
and is automatically rebuilt when changes are added to the main
repository.

Development of \dengo{} will be conducted in an identical manner as
\grackle{}.  Like \grackle{}, \dengo{} is also developed using
Mercurial and its main repo is hosted on
BitBucket.org\footnote{\url{https://bitbucket.org/dengo-project/dengo}}.

\subsection{Provenance and Reproducibility}

\mjt{Do we want to add anything here about reaction rate provenance?}

Reproducibility is integral to the scientific process, but can be
difficult to achieve when a calculation relies on a complex software
stack where dependency versions change frequently
\citep{2014arXiv1412.5557J, 2016arXiv161009958L}.  Increased
community involvement in a project, quickening the churn within the
source code, only exacerbates the situation.  In \grackle{}, this
problem is solved by building unique version identifiers directly
into the library.  When the library is compiled, the Mercurial
changeset hash (an effectively unique series of alpha-numeric
characters) is baked into a routine that outputs relevant information
both to a file, called ``GRACKLE\_INFO'', and to the terminal (via
STDOUT) whenever the library's initialize function is called.  In
addition to the changeset hash, the following information is written:
a ``diff'' of any uncommitted changes made to the source, the release
version, all compilation options, and all run-time parameters.
Additionally, \grackle{} maintains a ``CITATION'' file within the
source and in the documentation describing the proper citation
language and BibTex entries to the citable entities.  We will also
follow external examples, where appropriate, to produce machine
readable provenance information \citep[e.g.,][]{force11,
  Fenner097196}.  In these ways, \grackle{} serves as an example to
the community of mechanisms for making reproducibility documents.

\subsection{Alternative Software Packages}

There is a dearth of cross-platform chemistry and cooling packages 
packages available to the astrophysical simulation community.  This
fact was the primary motivator for \grackle{}'s creation.  The main
alternative to \grackle{} is the 
\texttt{KROME}\footnote{\url{http://kromepackage.org/}} package
\citep{2014MNRAS.439.2386G}.  \texttt{KROME} is functionally very
similar to \dengo{} in that it can generate implicit solver code for a
user-defined network.  \texttt{KROME} also exposes a number of
distinct chemical networks through a Fortran-based API.  However, this
API varies for each network, so a user must add support for them
individually.  Additionally, \texttt{KROME} is designed exclusively as
a chemistry network solver and so does not employ cost-saving
simplifications, like tabulated metal cooling and self-shielding
models, designed to work in regimes where direct network solving is
computationally prohibitive.  This is an important point as these
methods are widely used in the galaxy formation simulation community.
Finally, \texttt{KROME}'s solvers are designed to operate on a single
fluid element and do not offer additional parallelization;
\texttt{KROME} is not specifically targeted toward large simulations
or advancements in hardware acceleration.

Several other codes exist for solving specific chemistry
networks on single fluid elements, such as 
\texttt{ALCHEMIC} \citep{2010A&A...522A..42S}; \texttt{ASTROCHEM}
\citep{2013MNRAS.431..455K}; \texttt{ASTROCHEM} \citep[][unrelated to
the first \texttt{ASTROCHEM}]{2013A&A...559A..53M}; \texttt{NAHOON}
\citep{2012ApJS..199...21W}; \texttt{RATE13} \citep[associated with
  the UMIST database][]{2013A&A...550A..36M}; and \texttt{XDELOAD}
\citep{2005Ap&SS.299....1N}.  The \texttt{ASTROCHEM} by
\citet{2013A&A...559A..53M} is the only code hosted in a
publicly-viewable repository.  \texttt{NAHOON} and \texttt{RATE13} are
downloadable as tatic tar files.  \texttt{ALCHEMIC} and
\texttt{XDELOAD} are available upon request.  \texttt{ASTROCHEM} by
\citet{2013MNRAS.431..455K} is a private code.
In addition to the above codes, a number of works have provided tables
of cooling rates \citep{1993ApJS...88..253S, 2009MNRAS.393...99W,
2013MNRAS.434.1043O} that require the surrounding machinery for
interpolating and updating the internal energy to be implemented
independently.
These codes and tables supply important functionality, but it is
not their aim to provide a flexible API to large simulations, nor to
act as a two-way resource that both serves and invites contribution
from the larger community.

\bds{We could also mention a few of the various reaction networks for
  which no code is available or easy to obtain.  We could then say
  that these could all be used with Dengo.}

The \grackle{} project has focused on providing optimized, easily
adoptable functionality that is independent of simulation code
design.  Through its interoperability with other cross-platform
astrophysical tools like \yt{}, it seeks to exist within the
ecosystem of scientific software rather than alongside it.  The
\dengo{} code will adhere to this same philosophy.
\grackle{'s} focus on usability, performance, and community
involvement make it a vital software element to the field of
computational astrophysics.  This focus is underscored by the goals
of the work outlined in this proposal:
to make the code inviting to new contribution; to enhance performance
to scale with next-generation simulations; and to add features desired
by its current users as well as to make it useful to new communities.

\subsection{Advisory Board}

We have formed an advisory board to ensure that the milestones
outlined in this proposal are accomplished satisfactorily and in a timely
fashion.  The PIs will meet regularly (between 1 and 3 times
per year, via video-conference) with the full board to present updates
on technical progress and progress in achieving the overall goals of
the project in terms of growth in users and community involvement.  The
advisory board members are experts in the fields of computational
astrophysics and astrochemistry and have significant experience in the
development (both technical and social) of community software projects
and in parallelizing codes for GPUs.
{\bf Prof. Greg Bryan} is the original author of the \texttt{Enzo}
simulation code and provides experience with chemistry solving methods
and design of large software projects.
{\bf Dr. Anshu Dubey} was an associate director of the Flash Center
for computational science and one of the lead developers of the
\texttt{FLASH} simulation code \citep{2000ApJS..131..273F,
  2009arXiv0903.4875D}.  Dr. Dubey brings expertise in developing
large adaptive mesh-refinement codes and can advise on adding
\grackle{} support to \texttt{FLASH}.
{\bf Dr. Simon Glover} is a foremost expert in astrochemistry and
chemical networks and has contributed numerous updated chemical rates to
\grackle{}.  Dr. Glover will provide expertise in creating chemical
networks and evaluating their scientific applicability.
{\bf Prof. Michael Norman} is the head of the San Diego Supercomputer
Center and has coordinated development of \texttt{Enzo} and
\texttt{CELLO}/\texttt{Enzo-P}, a peta-scale AMR framework and
simulation code.  Prof. Norman provides experience in software design for
high performance computing and accelerating codes with GPUs and MIC
systems.
{\bf Prof. Stella Offner} is an expert in simulations of present-day
star formation and provides insight into the needs of that community.
Prof. Offner will also advise on adding support for \grackle{} to the
\texttt{ORION} simulation code \citep{2012ApJ...745..139L}.

The advisory board provides a range of scientific and technical
expertise as well as experience with community software development.
Their input will help to see that milestones are met and that
the project satisfies the needs of the communities it seeks to serve.
